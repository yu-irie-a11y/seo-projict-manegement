# 内部リンク構造 可視化ツール — 開発マニュアル

## 目次

1. [ツール作成の経緯](#1-ツール作成の経緯)
2. [構築の手順（作り方のマニュアル）](#2-構築の手順作り方のマニュアル)
3. [システムのメカニズムと基礎知識](#3-システムのメカニズムと基礎知識)
4. [V2 開発の経緯 — 主眼と改善点](#4-v2-開発の経緯--主眼と改善点)

---

## 1. ツール作成の経緯

### 1-1. きっかけ

金光八尾中学校・高等学校（DM000031_関西金光学園）のSEO施策として、全72ページのリンク設計書とサイトマップ改善案を作成した。これらの設計書をもとに「どのページからどのページにリンクが繋がっているか」を俯瞰的に把握するための可視化が必要になった。

### 1-2. 課題

- 72ページ × 各ページ数十件のリンク = **合計300〜400件**のリンク関係がある
- テーブル（表）形式だけでは全体の構造を把握しにくい
- 手動でMiroやFigJamに配置するのは非現実的な作業量
- Mermaid記法は50ノード超でレンダリングが破綻する

### 1-3. 解決アプローチの変遷

| ステップ | 内容 | 成果物 |
|---|---|---|
| **Step 1** | リンク設計書のテーブルデータを手動でJSON化し、D3.jsで力学レイアウト図を生成 | `金光八尾_リンク構造図_2026-02-07.html` |
| **Step 2** | 「任意のURLリストを渡すだけで自動生成」できるよう汎用ツール化 | `analyze-links.js` |
| **Step 3** | 実際のサイト（extage-marketing.co.jp、135記事）で動作検証 | `extage-link-map.html` |

### 1-4. Step 1 → Step 2 の発展ポイント

Step 1（金光八尾版）は設計書から**手動で**リンクデータを抽出してHTMLに埋め込んだ。この作業の中で「データさえ揃えば可視化部分は完全に自動化できる」と判明したため、Step 2 として**データ収集部分もクローラーで自動化**し、再現性のあるツールに昇格させた。

```
【Step 1: 手動フロー】
リンク設計書(MD) → 人間がJSON化 → D3.js HTML

【Step 2: 自動フロー】
URLリスト(TXT) → クローラーが自動取得 → パーサーがaタグ抽出 → D3.js HTML
```

---

## 2. 構築の手順（作り方のマニュアル）

### 2-1. 前提環境

| 項目 | 要件 |
|---|---|
| Node.js | v18以上（native fetch が必要） |
| npm パッケージ | `cheerio`（HTML解析用） |
| OS | macOS / Windows / Linux いずれもOK |

### 2-2. 初期セットアップ

```bash
# 1. プロジェクトディレクトリに移動
cd /path/to/your-project

# 2. package.json がなければ作成
npm init -y

# 3. cheerio をインストール
npm install cheerio
```

インストール後の `package.json`:
```json
{
  "dependencies": {
    "cheerio": "^1.2.0"
  }
}
```

### 2-3. ファイル構成

```
project/
├── analyze-links.js      ← メインスクリプト（ツール本体）
├── urls-extage.txt       ← 入力：解析対象のURLリスト
├── extage-link-map.html  ← 出力：生成される可視化HTML
├── package.json
└── node_modules/
```

### 2-4. 使い方

#### 基本

```bash
node analyze-links.js urls.txt
```

#### オプション付き

```bash
# 出力ファイル名を指定
node analyze-links.js urls.txt --output report.html

# リクエスト間隔を2秒に（サーバー負荷軽減）
node analyze-links.js urls.txt --delay 2000

# 両方指定
node analyze-links.js urls.txt -o client-report.html -d 1500
```

#### URLリストの書き方

`urls.txt` に1行1URLで記載する。`#` で始まる行はコメントとして無視される。

```
# クライアントA - コーポレートサイト
https://example.com/
https://example.com/about/
https://example.com/service/
https://example.com/blog/article-1/
https://example.com/blog/article-2/
```

### 2-5. URLリストの収集方法

解析対象のURLリストを効率的に集める方法は3つある。

#### 方法A: サイトマップから取得（最も確実）

ほとんどのWebサイトには `/sitemap.xml` がある。ここから対象URLを一括取得できる。

```
https://example.com/sitemap.xml
https://example.com/post-sitemap.xml    ← WordPress の場合
```

ブラウザでアクセスし、表示されたURLをコピーしてテキストファイルに貼り付ける。

#### 方法B: Google Search Console から取得

Search Console > パフォーマンス > ページタブ から、インデックス済みURLを一覧エクスポートできる。

#### 方法C: Screaming Frog 等のクローラーで取得

Screaming Frog SEO Spider で対象サイトをクロールし、内部URLリストをCSVエクスポートする。

### 2-6. 出力HTMLの使い方

生成された `.html` ファイルをブラウザで開くと、インタラクティブなリンク構造図が表示される。

| 操作 | 効果 |
|---|---|
| **マウスホバー** | ページ名、パス、被リンク数、発リンク数をツールチップ表示 |
| **ノードクリック** | 接続先をハイライト＋右パネルにリンク一覧（アンカーテキスト付き） |
| **背景クリック** | ハイライト解除 |
| **ドラッグ** | ノードを手動で移動 |
| **スクロール** | ズームイン/アウト |
| **検索ボックス** | ページ名・URLでリアルタイム検索 |
| **フィルター** | 左パネルのセクション名をクリックで表示/非表示 |

### 2-7. SEO分析での活用ポイント

出力結果を見るときに、以下の観点でチェックすると内部リンク改善に直結する。

| チェック観点 | 見方 | 改善アクション |
|---|---|---|
| **孤立ページ** | 線が1〜2本しかないノード | 関連記事からの内部リンクを追加 |
| **ハブページの偏り** | 特定ノードに線が集中しすぎ | リンクジュースの分散を検討 |
| **クラスター構造** | 同カテゴリの記事がまとまっているか | トピッククラスター設計の見直し |
| **CTA導線** | お問い合わせ・資料請求への線の量 | CVポイントへの導線が薄い記事を特定 |
| **点線ノード** | 未クロール（URLリストに含まれていないリンク先） | カテゴリページ等、必要なら追加クロール |

---

## 3. システムのメカニズムと基礎知識

### 3-1. 全体アーキテクチャ

ツールは以下の4つの処理フェーズで構成される。

```
┌─────────────────────────────────────────────────────────┐
│                                                         │
│  Phase 1: 入力                                          │
│  ┌─────────┐                                            │
│  │urls.txt │ → URLリストを読み込み                      │
│  └─────────┘   コメント行(#)除去、空行除去              │
│       ↓                                                 │
│  Phase 2: クロール                                      │
│  ┌─────────────────────────┐                            │
│  │ fetch() で各URLのHTML取得│                            │
│  │ ・User-Agent偽装        │                            │
│  │ ・タイムアウト制御      │                            │
│  │ ・レート制限(delay)     │                            │
│  └─────────────────────────┘                            │
│       ↓                                                 │
│  Phase 3: パース＆フィルタリング                         │
│  ┌─────────────────────────┐                            │
│  │ cheerio でHTML解析      │                            │
│  │ ・全 <a href=""> を抽出 │                            │
│  │ ・相対URL→絶対URL変換   │                            │
│  │ ・外部リンク除外        │                            │
│  │ ・PDF等のファイル除外   │                            │
│  │ ・ハッシュ(#)除去       │                            │
│  │ ・同一ページ重複除去    │                            │
│  │ ・アンカーテキスト取得  │                            │
│  └─────────────────────────┘                            │
│       ↓                                                 │
│  Phase 4: グラフ構築＆HTML生成                           │
│  ┌─────────────────────────┐    ┌──────────────────┐   │
│  │ ノード(ページ)構築      │ →  │ D3.js HTMLを生成 │   │
│  │ エッジ(リンク)構築      │    │ データをJSONで埋込│   │
│  │ グループ自動推定        │    │ ファイル書き出し  │   │
│  │ 被リンク/発リンク集計   │    └──────────────────┘   │
│  └─────────────────────────┘                            │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### 3-2. Phase 2: クロールのメカニズム

#### fetch API（Node.js v18+ ネイティブ）

Node.js v18 以降に標準搭載された `fetch()` を使用。外部パッケージ不要でHTTPリクエストを送信できる。

```javascript
const res = await fetch(url, {
  headers: { "User-Agent": USER_AGENT },
  signal: controller.signal,  // タイムアウト制御
  redirect: "follow",         // リダイレクトを自動追跡
});
const html = await res.text();
```

**設計上の重要ポイント：**

| 項目 | 実装 | 理由 |
|---|---|---|
| User-Agent | Chrome のUAを指定 | 一部サイトがボットを弾くため |
| タイムアウト | 15秒 | 応答しないサーバーで処理が止まるのを防止 |
| リクエスト間隔 | デフォルト1秒 | サーバーに過負荷をかけないため |
| リダイレクト | 自動追跡 | 301/302 リダイレクト先のHTMLを取得するため |
| Content-Type | text/html のみ処理 | PDF・画像等のバイナリをスキップ |

#### レート制限

```javascript
// 各リクエスト間に指定ミリ秒のスリープを挿入
await new Promise(r => setTimeout(r, delay));
```

これにより、100ページのサイトでも `delay=1000` なら約2分、`delay=2000` なら約4分で完了する。

### 3-3. Phase 3: HTMLパースのメカニズム

#### cheerio ライブラリ

cheerio は「サーバーサイドのjQuery」と呼ばれるHTML解析ライブラリ。ブラウザを起動せずにHTMLのDOM操作ができる。

```javascript
const { load } = require("cheerio");
const $ = load(html);

// 全 <a> タグから href 属性を取得
$("a[href]").each((_, el) => {
  const href = $(el).attr("href");       // リンク先URL
  const text = $(el).text().trim();       // アンカーテキスト
});
```

#### URL正規化の処理

Webページのリンクは様々な形式で記述されているため、統一的に処理する必要がある。

```
【入力例】                    【正規化後】
href="/about"              → https://example.com/about/
href="./service/"          → https://example.com/service/
href="../contact"          → https://example.com/contact/
href="#section1"           → (除外)
href="javascript:void(0)" → (除外)
href="mailto:info@..."    → (除外)
href="tel:0120-..."       → (除外)
```

実装では `new URL(href, baseUrl)` でブラウザと同じURL解決ロジックを使用している。

```javascript
function normalizeUrl(href, baseUrl) {
  // 無効なリンクを除外
  if (!href || href.startsWith("javascript:") || href.startsWith("mailto:")) return null;

  // 相対URL→絶対URLに変換（ブラウザと同じロジック）
  const resolved = new URL(href, baseUrl);

  // ハッシュ(#)を除去
  resolved.hash = "";

  return resolved.href;
}
```

#### 内部リンク判定

「同一ドメインかどうか」で内部/外部を自動判定する。

```javascript
// 入力URLリストからドメインを自動収集
const targetDomains = new Set();
urls.forEach(url => targetDomains.add(new URL(url).hostname));

// リンク先のドメインが targetDomains に含まれるかで判定
const linkDomain = new URL(normalized).hostname;
if (!targetDomains.has(linkDomain)) return; // 外部リンク → 除外
```

### 3-4. Phase 4: 可視化のメカニズム

#### D3.js Force-Directed Graph（力学シミュレーション）

生成されるHTMLの可視化エンジンは **D3.js v7 の力学シミュレーション** を使用している。

```
┌────────────────────────────────────────────────┐
│             力学モデルの概念                     │
│                                                │
│  各ノード = 「電荷を持つ粒子」                  │
│  各エッジ = 「バネ」                            │
│                                                │
│  ・ノード同士は反発力で離れようとする           │
│  ・リンクで繋がったノードはバネで引き寄せ合う   │
│  ・中心に向かう弱い引力がある                   │
│  ・衝突回避の力がある                           │
│                                                │
│  → これらの力が平衡状態になるまで               │
│    シミュレーションを繰り返す                   │
│  → 自然とクラスターが形成される                 │
└────────────────────────────────────────────────┘
```

D3.js の力学パラメータ：

```javascript
d3.forceSimulation(nodes)
  .force("link",      d3.forceLink(links).distance(80))    // バネの長さ
  .force("charge",    d3.forceManyBody().strength(-300))    // 反発力の強さ
  .force("center",    d3.forceCenter(width/2, height/2))   // 中心への引力
  .force("collision", d3.forceCollide().radius(25))         // 衝突回避半径
```

| パラメータ | 値 | 効果 |
|---|---|---|
| `link.distance` | 80 | リンクで繋がったノード間の理想距離（px） |
| `charge.strength` | -300 | 負の値 = 反発力。絶対値が大きいほどノードが散らばる |
| `center` | 画面中央 | ノード群全体が画面中央に留まる力 |
| `collision.radius` | 25 | ノード同士が重ならないための最小距離 |

**ノード数に応じた自動調整：**

```javascript
// ノードが多いほど反発力を弱め、リンク距離を縮めて画面に収める
const chargeStrength = nodes.length > 80 ? -200 : nodes.length > 40 ? -300 : -400;
const linkDist = nodes.length > 80 ? 60 : 80;
```

#### ノードの視覚表現

```
ノードの大きさ = 被リンク数に比例
  被リンク0件  → 半径 4px
  被リンク10件 → 半径 ~13px
  被リンク30件 → 半径 ~20px（最大22px）

ノードの色 = URLパスの第1セグメントから自動推定したグループ
  /web-school/... → "web-school" グループ → 色A
  /services/...   → "services" グループ   → 色B
  /blog/...       → "blog" グループ       → 色C

ノードの枠線
  実線 = クロール済みページ（URLリストに含まれていた）
  点線 = 未クロールページ（リンク先として検出されたが、URLリストに未記載）
```

#### SVGによる描画

HTMLの `<svg>` 要素内に以下の構造で描画される。

```html
<svg>
  <g class="container">     ← ズーム・パン用のコンテナ
    <g class="links">       ← リンク線（<line> 要素）
      <line x1="..." y1="..." x2="..." y2="...">
    </g>
    <g class="nodes">       ← ノード（<g> > <circle> + <text>）
      <g transform="translate(x, y)">
        <circle r="10" fill="#4a9eff">
        <text>ページ名</text>
      </g>
    </g>
  </g>
</svg>
```

### 3-5. グループ自動推定ロジック

URLのパス構造からセクションを自動推定し、色分けに使用する。

```javascript
function guessGroup(url) {
  const pathname = new URL(url).pathname;
  const firstSegment = pathname.split("/").filter(Boolean)[0];

  // パターンマッチング
  if (firstSegment.match(/blog|news|column/))   return "blog";
  if (firstSegment.match(/about|company/))       return "about";
  if (firstSegment.match(/service|product/))     return "service";
  if (firstSegment.match(/contact|inquiry/))     return "contact";
  // ... その他のパターン

  // マッチしない場合はパス名をそのままグループ名に
  return firstSegment;  // 例: "web-school"
}
```

20色のパレットからグループ数に応じて自動割り当てされる。

### 3-6. データフローの全体像

```
urls.txt
  │
  ▼
["https://example.com/", "https://example.com/about/", ...]
  │
  ▼ (fetch + cheerio)
[
  { url: "https://example.com/",
    title: "会社名 | トップページ",
    links: [
      { target: "https://example.com/about/", anchorText: "会社概要" },
      { target: "https://example.com/service/", anchorText: "サービス" },
    ]
  },
  { url: "https://example.com/about/",
    title: "会社概要 | 会社名",
    links: [
      { target: "https://example.com/", anchorText: "トップへ" },
    ]
  },
  ...
]
  │
  ▼ (グラフ構築)
nodes = [
  { id: "https://example.com/", label: "会社名 | トップページ", group: "top" },
  { id: "https://example.com/about/", label: "会社概要 | 会社名", group: "about" },
  ...
]
edges = [
  { source: "https://example.com/", target: "https://example.com/about/", anchorText: "会社概要" },
  { source: "https://example.com/about/", target: "https://example.com/", anchorText: "トップへ" },
  ...
]
  │
  ▼ (HTMLテンプレートにJSONとして埋め込み)
<script>
  const nodes = [{"id":"https://example.com/","label":"会社名 | トップページ",...}];
  const links = [{"source":"https://example.com/","target":"https://example.com/about/",...}];
</script>
  │
  ▼
link-map.html（ブラウザで開くとD3.jsが描画）
```

### 3-7. 技術スタックまとめ

| レイヤー | 技術 | 役割 |
|---|---|---|
| ランタイム | Node.js v18+ | スクリプト実行環境 |
| HTTP | fetch (ネイティブ) | Webページの取得 |
| HTML解析 | cheerio | aタグ等のDOM解析 |
| 可視化エンジン | D3.js v7 (CDN) | 力学シミュレーション＋SVG描画 |
| 出力形式 | 単一HTML | ブラウザだけで動作、サーバー不要 |

### 3-8. 制限事項と注意点

| 制限 | 原因 | 対策 |
|---|---|---|
| JavaScript で動的生成されるリンクは取得不可 | cheerio は静的HTMLのみ解析 | Puppeteer等のヘッドレスブラウザに差し替え |
| ログインが必要なページは取得不可 | 認証Cookie未送信 | Cookie オプションの追加実装 |
| 500ページ超は可視化が重くなる | D3.jsの描画負荷 | フィルターで表示数を絞る / WebGL描画に切替 |
| 同一ページ内の重複リンクは1件に集約 | 意図的な仕様 | 集約せずカウントしたい場合はコード修正 |

---

## 付録

### A. 実行ログの例（extage-marketing.co.jp 135記事）

```
===== 内部リンク構造 解析ツール =====

対象URL: 135 件
リクエスト間隔: 1000ms
出力先: extage-link-map.html

対象ドメイン: www.extage-marketing.co.jp

[1/135] https://www.extage-marketing.co.jp/web-school/access-increasing-method/ ... ✓ 41 リンク
[2/135] https://www.extage-marketing.co.jp/web-school/administrative-scrivener... ... ✓ 36 リンク
...（中略）...
[135/135] https://www.extage-marketing.co.jp/web-school/ymyl-genre/ ... ✓ 32 リンク

--- 解析結果 ---
ノード数: 217
エッジ数: 4,998

被リンク TOP10:
  1. (132件) /web-school/
  2. (132件) /services/
  ...
```

### B. ファイル一覧

| ファイル | 説明 |
|---|---|
| `analyze-links.js` | ツール本体（Node.jsスクリプト） |
| `urls-extage.txt` | extage 135記事のURLリスト |
| `extage-link-map.html` | extage の解析結果（可視化HTML） |
| `urls-sample.txt` | テスト用サンプルURLリスト |
| `link-map-test.html` | テスト実行の出力 |
| `金光八尾_リンク構造図_2026-02-07.html` | Step 1 で手動作成した金光八尾の構造図 |
| `内部リンク解析ツール_マニュアル.md` | 本マニュアル |

### C. 今後の拡張アイデア

| アイデア | 概要 |
|---|---|
| **サイトマップ自動取得** | URLリストの手動作成を省略。`/sitemap.xml` を自動解析して全ページURLを取得 |
| **グローバルナビ除外フィルター** | 全ページ共通のヘッダー/フッターリンクを自動検出して除外し、記事本文内のリンクだけを可視化 |
| **アンカーテキスト分析** | リンクの文言を集計し、「詳しくはこちら」等のSEO的に弱いアンカーテキストを警告 |
| **孤立ページ検出レポート** | 内部リンクが0〜1件しかないページを自動抽出してCSV出力 |
| **定期実行＆差分比較** | 月次で実行し、前回との差分（新規リンク/削除リンク）をレポート |
| **ヘッドレスブラウザ対応** | Puppeteer を使いJavaScript生成のリンクも取得可能にする |

---

## 4. V2 開発の経緯 — 主眼と改善点

### 4-1. V1 → V2 で何に主眼を置いたか

V1（初版）はURLリストからリンク構造図を生成するところまでは実現したが、実際にextage 135記事で動かすと**実用性に大きな課題**があった。V2ではこの課題を1つずつ潰す形で機能を積み重ねた。

#### (A) ノイズの除去 — 本文リンクだけを見る

V1は`<a>`タグをページ全体から抽出していたため、全ページ共通のヘッダー/フッター/サイドバーのリンクが大量にエッジとして入り込み、**エッジ数が4,998本**に膨れ上がった。どのリンクが記事本文の意図的な内部リンクなのか判別不能だった。

V2では**メインコンテンツ領域のみ**からリンクを抽出するフィルタリングを導入。`header`, `footer`, `nav`, `aside`, `[role="navigation"]`等を除外し、`main`, `article`, `.entry-content`等の本文領域に絞り込んだ。

```
V1: 4,998 エッジ（ヘッダー/フッター含む）
V2: 1,606 エッジ（本文リンクのみ） → 68%削減
```

**この判断の背景**: SEOにおける内部リンク最適化は「記事本文からの意図的な導線」を設計するもの。グローバルナビやフッターの共通リンクはサイト構造として重要だが、記事単位の内部リンク分析には**ノイズ**になる。

#### (B) 特定ページへの到達性 — 検索機能の強化

200ノード超のグラフビューでは「あの記事どこにあるんだ？」という問題が頻発した。検索ボックスは最初からあったが、V2ではドロップダウン付きの検索結果一覧に強化。

- 検索結果に被リンク/発リンク数を表示
- 矢印キー（↑↓）で候補を選択、Enterで即座にズーム
- クリックでも直接ノードへ遷移
- 接続ノードをハイライトして関係性を可視化

**この判断の背景**: 実務では「被リンクが少ない記事Xを見つけて、関連する記事Yから内部リンクを追加する」という**ピンポイント作業**が多い。グラフを眺めるだけでなく「目的の記事に素早く辿り着く」UIが必要だった。

#### (C) 操作性 — トラックパッド対応

MacBookのトラックパッドで操作したとき、D3.jsのデフォルト挙動では2本指スクロールがズームになってしまい、パン（平行移動）ができなかった。

V2ではD3のデフォルトwheel処理を無効化し、自前で以下のように振り分けた:

| 操作 | 挙動 |
|---|---|
| 2本指スクロール | パン（平行移動） |
| ピンチ（Ctrl+スクロール） | ズーム |

**この判断の背景**: 200ノード超のグラフでは頻繁にパン+ズームを繰り返す。トラックパッド操作が自然でないと、数十分の分析作業でストレスが溜まる。

#### (D) ページ間遷移 — グラフ内ナビゲーション + URL直接アクセス

ノードをクリックして右パネルに被リンク・発リンク一覧が出るところまではV1にもあったが、V2では2つの遷移手段を追加。

1. **グラフ内遷移**: リンク一覧の行をクリック → そのノードへズーム＋ハイライト
2. **ページを開く（↗）**: `↗`ボタンで実際のWebページを別タブで開く

**この判断の背景**: 分析中に「このリンク元の記事は実際どんな内容なのか？」を確認したくなる場面が非常に多い。グラフ上の遷移と実ページの確認を**シームレスに行き来**できることが重要。

#### (E) 被リンク分布の網羅的把握 — 分析パネルの再設計

V1の分析パネルは被リンク0件と1〜2件のページのみリストアップしていた。V2では**全ページを被リンク数の昇順で網羅的に表示**する形に再設計。

- 被リンク数ごとのグループヘッダー（0件, 1件, 2件, 3件, ...）
- サマリーカード: 0件(赤), 1〜3件(橙), 4〜10件(青), 11件+(緑)
- バーグラフでページ数分布を可視化
- 各ページはクリックでグラフ遷移、↗で別タブ

**この判断の背景**: 「被リンク0件だけ」を直しても不十分。被リンク3件の記事は本当に足りているのか？ 5件で十分か？ 全体の分布を見ることで「どこまで手を入れるべきか」の判断基準ができる。

### 4-2. 次回制作する際に改善すべき点

V2の開発プロセスを振り返り、次回同様のツールを作る際に意識すべきポイントをまとめる。

#### 改善1: テンプレートリテラル内のエスケープ検証を仕組み化する

**起きた問題**: 分析パネルのHTML生成で`onmouseover="this.style.color='#4a9eff'"`のようなインラインイベント属性を書いた際、テンプレートリテラル→JS文字列→HTML属性の**3重エスケープ**が噛み合わず、出力HTMLが構文エラーで表示不能になった。

**原因**: analyze-links.jsはNode.jsのテンプレートリテラル（バッククォート）で巨大なHTML文字列を組み立てており、その中のJSコード内で`h+='...'`のシングルクォート文字列がある。テンプレートリテラル内の`\'`はただの`'`になるため、出力先のJS文字列を壊してしまう。

**対策**:
- HTML属性内のクォートには`&#39;`(シングルクォート)や`&quot;`(ダブルクォート)のHTMLエンティティを使う
- **出力HTMLの構文チェックをスクリプトに組み込む**。生成後に`new Function(scriptContent)`で自動検証し、エラーがあれば出力前に警告する仕組みを追加する
- 理想的にはHTML生成部分をテンプレートファイルに分離し、テンプレートリテラルの入れ子を減らす

#### 改善2: 機能追加のたびに即座にブラウザ確認する

**起きた問題**: 検索強化→トラックパッド対応→詳細パネル改修→分析パネル再設計、と連続で機能追加した後にまとめてHTMLを再生成したため、構文エラーの発見が最後になった。どの変更が壊したのか特定に時間がかかった。

**対策**: 機能を1つ追加するたびに`node analyze-links.js`で再生成→ブラウザ確認のサイクルを回す。ただし135ページのフェッチに2分以上かかるため、以下の短縮策も併用する:
- テスト用に5〜10ページの小規模URLリストを用意しておく
- 一度フェッチしたデータをJSONキャッシュとして保存し、HTML生成だけを高速に繰り返せるようにする

#### 改善3: フェッチ結果のキャッシュ機構

**起きた問題**: コード修正→再生成のたびに135ページを毎回フェッチし直す必要があり、1回2分以上待つ。開発効率が大きく下がった。

**対策**: クロール済みHTMLをローカルにJSONキャッシュし、`--cache`オプションで再利用できるようにする。HTMLテンプレート修正だけなら0.5秒で再生成可能になる。

```
node analyze-links.js urls.txt --cache ./cache.json    # 初回: フェッチ+キャッシュ保存
node analyze-links.js urls.txt --cache ./cache.json    # 2回目以降: キャッシュ利用
node analyze-links.js urls.txt --no-cache              # キャッシュ無視で再フェッチ
```

#### 改善4: HTMLテンプレートの分離

**起きた問題**: analyze-links.jsの中にHTMLテンプレート（CSS + HTML + JS）がテンプレートリテラルとして直接埋め込まれており、800行超の巨大ファイルになっている。エスケープ問題の根本原因でもある。

**対策**: HTMLテンプレートを別ファイル（例: `template.html`）に分離し、`${DATA_PLACEHOLDER}`のようなプレースホルダーだけをJSONデータで置換する構造にする。これにより:
- HTMLテンプレートはそのままブラウザでプレビューできる
- エスケープ問題がなくなる
- テンプレート単体のlint/バリデーションが可能

#### 改善5: テスト用の小規模データセットを最初から用意する

**起きた問題**: 135記事でしか動作確認ができず、テストのたびに待ち時間が発生。

**対策**: 5〜10記事の小規模URLリスト（`urls-test.txt`）をプロジェクト初期から用意し、開発中はそちらで高速に動作確認する。本番URLリストでの確認は最終段階のみ。

### 4-3. V1 → V2 の変更サマリー

| 項目 | V1 | V2 |
|---|---|---|
| リンク抽出範囲 | ページ全体 | メインコンテンツのみ（header/footer/nav除外） |
| エッジ数(extage) | 4,998 | 1,606（68%削減） |
| 検索 | テキスト入力のみ | ドロップダウン＋キーボード操作＋IN/OUT表示 |
| トラックパッド | D3デフォルト（ズームのみ） | 2本指パン＋ピンチズーム |
| 詳細パネル | テキスト表示 | クリックでグラフ遷移＋↗で別タブ |
| 分析パネル | 被リンク0〜2件のみ表示 | 全ページ被リンク数昇順＋サマリー＋バーグラフ |
| HTML出力 | `link-map.html`固定 | `-o`オプションで任意ファイル名 |
| 出力オプション | なし | `--all-links`で全リンク抽出モード追加 |

---

**作成日**: 2026年2月7日
**最終更新**: 2026年2月7日（V2対応追記）
**作成者**: SEOプロジェクト管理チーム
